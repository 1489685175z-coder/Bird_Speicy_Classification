{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmYP5PbNFGxHBwfbh9QV4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1489685175z-coder/Bird_Speicy_Classification/blob/main/Bird_Speicy_Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bird_classification.py\n",
        "# Project: Fine-Grained Bird Species Classification on CUB-200-2011\n",
        "# Model: Pre-trained ResNet-18 fine-tuned\n",
        "# Dataset: bentrevett/caltech-ucsd-birds-200-2011 from Hugging Face\n",
        "# Author: Adapted for CSCI218 assignment\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "SAhzoagBUzvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. Device & Hyperparameters\n",
        "# =============================================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 64             # Increase to 64~96 if GPU memory allows\n",
        "NUM_EPOCHS = 15             # Adjust based on convergence (10~20 is usually enough)\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_CLASSES = 200           # CUB-200 has 200 bird species"
      ],
      "metadata": {
        "id": "pdR2kEouUz8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Load Dataset from Hugging Face\n",
        "# =============================================================================\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset from Hugging Face...\")\n",
        "dataset = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\")\n",
        "\n",
        "# Custom Dataset wrapper\n",
        "class CUBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.hf_dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.hf_dataset[idx]\n",
        "        image = item['image'].convert('RGB')\n",
        "        label = item['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CUBDataset(dataset['train'], transform=train_transform)\n",
        "test_dataset  = CUBDataset(dataset['test'], transform=test_transform)\n",
        "\n",
        "# Split train into train + val\n",
        "train_size = int(0.85 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}, Test samples: {len(test_dataset)}\")\n",
        "print(\"Data loading completed.\")"
      ],
      "metadata": {
        "id": "dSLuNgbqUz-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Model Definition\n",
        "# =============================================================================\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: Freeze early layers for faster & more stable fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 and fc (last residual block + classifier)\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "ipaxrsETU0BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Loss Function & Optimizer\n",
        "# =============================================================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)"
      ],
      "metadata": {
        "id": "f2dUGxY4U0D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Training Function\n",
        "# =============================================================================\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), correct / total"
      ],
      "metadata": {
        "id": "H6Eie9QPU0GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 7. Evaluation Function\n",
        "# =============================================================================\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "MLIaNfblU0Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 8. Main Training Loop\n",
        "# =============================================================================\n",
        "best_val_acc = 0.0\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_acc = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Learning rate scheduler & save best model\n",
        "    scheduler.step(val_acc)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_resnet18_cub.pth\")\n",
        "        print(f\"  → Best model saved (Val Acc: {val_acc:.4f})\")"
      ],
      "metadata": {
        "id": "JVWV1YaZU0LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 9. Final Test Evaluation\n",
        "# =============================================================================\n",
        "print(\"\\nLoading best model for final test...\")\n",
        "model.load_state_dict(torch.load(\"best_resnet18_cub.pth\", map_location=device))\n",
        "test_acc = evaluate(model, test_loader, device)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "fJ35mgI1U0No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# Error Analysis & Confusion Matrix\n",
        "# =============================================\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Re-load the best model\n",
        "model = models.resnet18(pretrained=False)  # No pretrained here, we load our weights\n",
        "model.fc = nn.Linear(model.fc.in_features, 200)  # 200 classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the saved best checkpoint\n",
        "model.load_state_dict(torch.load(\"best_resnet18_cub.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Best model loaded for error analysis.\")\n",
        "\n",
        "# Collect all predictions and true labels from test set\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Collecting predictions for error analysis\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(f\"Collected {len(all_preds)} test predictions.\")\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(cm, annot=False, cmap='Blues', fmt='d')\n",
        "plt.title('Confusion Matrix on Test Set')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig('confusion_matrix.png')  # Save for report\n",
        "plt.show()\n",
        "\n",
        "# 2. Top 5 most confused pairs\n",
        "errors = [(true, pred) for true, pred in zip(all_labels, all_preds) if true != pred]\n",
        "if errors:\n",
        "    most_common_errors = Counter(errors).most_common(5)\n",
        "    print(\"Top 5 most confused pairs (true → predicted, count):\")\n",
        "    for (true_label, pred_label), count in most_common_errors:\n",
        "        print(f\"  {true_label} → {pred_label} : {count} times\")\n",
        "else:\n",
        "    print(\"No errors found (perfect prediction!).\")\n",
        "\n",
        "# Optional: Print overall test accuracy again\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Re-computed Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "OvOrSrgM0dEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def topk_accuracy(outputs, targets, topk=(1,5)):\n",
        "    \"\"\"\n",
        "    Compute top-k accuracy.\n",
        "    outputs: [batch, num_classes]\n",
        "    targets: [batch]\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "    _, pred = outputs.topk(maxk, dim=1, largest=True, sorted=True)  # [batch, maxk]\n",
        "    pred = pred.t()  # [maxk, batch]\n",
        "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append((correct_k / targets.size(0)).item())\n",
        "    return res\n",
        "\n",
        "model.eval()\n",
        "top1_total, top5_total, n_total = 0.0, 0.0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        t1, t5 = topk_accuracy(outputs, labels, topk=(1,5))\n",
        "        bs = labels.size(0)\n",
        "        top1_total += t1 * bs\n",
        "        top5_total += t5 * bs\n",
        "        n_total += bs\n",
        "\n",
        "print(f\"Test Top-1 Accuracy: {top1_total/n_total:.4f}\")\n",
        "print(f\"Test Top-5 Accuracy: {top5_total/n_total:.4f}\")"
      ],
      "metadata": {
        "id": "UUJIqsj2khfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion matrix shape:\", cm.shape)"
      ],
      "metadata": {
        "id": "2DO0sF8Bknpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Find indices of misclassified samples\n",
        "wrong_idx = np.where(all_preds != all_labels)[0]\n",
        "print(\"Number of misclassified samples:\", len(wrong_idx))\n",
        "\n",
        "# Display the first 9 misclassified images\n",
        "show_n = 9\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10,10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Collect images from test_loader\n",
        "images_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images_list.append(images)\n",
        "        labels_list.append(labels)\n",
        "images_all = torch.cat(images_list, dim=0)\n",
        "labels_all = torch.cat(labels_list, dim=0)\n",
        "\n",
        "for i in range(show_n):\n",
        "    idx = wrong_idx[i]\n",
        "    img = images_all[idx]\n",
        "\n",
        "    # # Denormalize (using ImageNet mean/std)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "    img = img * std + mean\n",
        "    img = img.permute(1,2,0).numpy()\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"True:{labels_all[idx].item()} Pred:{all_preds[idx]}\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RdU96QRQkr4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}