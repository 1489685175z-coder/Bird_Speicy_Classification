{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1489685175z-coder/Bird_Speicy_Classification/blob/main/Bird_Speicy_Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Project: Fine-Grained Bird Species Classification on CUB-200-2011\n",
        "# Compare: Pretrained ResNet-18 vs Pretrained ViT\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Install dependencies (if needed in Colab)\n",
        "# =============================================================================\n",
        "!pip install -q transformers datasets torch torchvision scikit-learn matplotlib pillow tqdm evaluate\n",
        "!pip install seaborn\n",
        "# =============================================================================\n",
        "# 2. Import libraries\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from transformers import ViTForImageClassification\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, top_k_accuracy_score, classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sRUFLM3ZOtZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644a7727-1b3e-4580-ab99-27a2480f786a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Device & Hyperparameters\n",
        "# =============================================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 32  # Compromise between originals (16 for ViT, 64 for ResNet)\n",
        "NUM_EPOCHS = 30   # Balanced for comparison\n",
        "NUM_CLASSES = 200\n",
        "\n",
        "models_config = [\n",
        "    {\"name\": \"resnet18\", \"pretrained\": True, \"lr\": 0.001},\n",
        "    {\"name\": \"vit\", \"pretrained\": True, \"lr\": 3e-5}\n",
        "]\n"
      ],
      "metadata": {
        "id": "LjrWG7zvOtWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec8fb20-c2ee-4372-b20f-78729a13df10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Load Dataset from Hugging Face\n",
        "# =============================================================================\n",
        "print(\"Loading dataset...\")\n",
        "hf_dataset = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\")\n",
        "\n",
        "class_names = hf_dataset['train'].features['label'].names  # Get class names for reporting\n",
        "\n",
        "class CUBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.hf_dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.hf_dataset[idx]\n",
        "        image = item['image'].convert('RGB')\n",
        "        label = item['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_train_ds = CUBDataset(hf_dataset['train'], train_transform)\n",
        "test_ds = CUBDataset(hf_dataset['test'], test_transform)\n",
        "\n",
        "train_size = int(0.85 * len(full_train_ds))\n",
        "val_size = len(full_train_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}, Test samples: {len(test_ds)}\")\n",
        "print(f\"Num classes: {NUM_CLASSES}, Example classes: {class_names[:5]}...\")\n"
      ],
      "metadata": {
        "id": "Q1f3vEbNOsxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc6bcb0-91f8-4000-e86d-5cf5201eba47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Train samples: 5094, Val samples: 900, Test samples: 5794\n",
            "Num classes: 200, Example classes: ['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Train and Evaluate Functions (Unified for both models)\n",
        "# =============================================================================\n",
        "def train_epoch(model, loader, criterion, optimizer, scaler, device, model_name):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            if model_name == \"vit\":\n",
        "                outputs = model(pixel_values=images).logits\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        pred = outputs.argmax(dim=1)\n",
        "        correct += (pred == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device, model_name, is_test=False):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels, all_logits = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            if model_name == \"vit\":\n",
        "                outputs = model(pixel_values=images).logits\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            pred = outputs.argmax(dim=1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            if is_test:\n",
        "                all_logits.extend(outputs.cpu().numpy())  # For top-5\n",
        "    acc = correct / total\n",
        "    loss = total_loss / total\n",
        "    top5_acc = top_k_accuracy_score(all_labels, all_logits, k=5) if is_test else None\n",
        "    return loss, acc, top5_acc, all_preds, all_labels\n"
      ],
      "metadata": {
        "id": "OT3WHvJOO2VE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Main Training Loop\n",
        "# =============================================================================\n",
        "all_history = {}\n",
        "test_results = {}\n",
        "for config in models_config:\n",
        "    name = config[\"name\"]\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    if name == \"resnet18\":\n",
        "        model = models.resnet18(pretrained=config[\"pretrained\"])\n",
        "        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "        # Freeze all except last layer and classifier (as in original ResNet)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in model.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    elif name == \"vit\":\n",
        "        model = ViTForImageClassification.from_pretrained(\n",
        "            \"google/vit-base-patch16-224-in21k\",\n",
        "            num_labels=NUM_CLASSES,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        # No freezing in original ViT, so train all with small LR\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=0.01)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"train_time\": []}\n",
        "    best_val_acc = 0.0\n",
        "    t_start = time.time()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device, name)\n",
        "        va_loss, va_acc, _, _, _ = evaluate(model, val_loader, criterion, device, name)\n",
        "        scheduler.step(va_acc)\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_loss\"].append(va_loss)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "        epoch_time = time.time() - t_start\n",
        "        history[\"train_time\"].append(epoch_time)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {tr_loss:.4f} Acc: {tr_acc:.4f} | Val Loss: {va_loss:.4f} Acc: {va_acc:.4f}\")\n",
        "\n",
        "        if va_acc > best_val_acc:\n",
        "            best_val_acc = va_acc\n",
        "            torch.save(model.state_dict(), f\"best_{name}.pth\")\n",
        "\n",
        "    total_train_time = time.time() - t_start\n",
        "    print(f\"Total training time for {name}: {total_train_time:.1f}s\")\n",
        "\n",
        "    # Load best model for test\n",
        "    model.load_state_dict(torch.load(f\"best_{name}.pth\", map_location=device))\n",
        "    te_loss, te_acc, te_top5_acc, te_preds, te_labels = evaluate(model, test_loader, criterion, device, name, is_test=True)\n",
        "    report = classification_report(te_labels, te_preds, target_names=class_names, output_dict=True)\n",
        "    macro_f1 = report['macro avg']['f1-score']\n",
        "\n",
        "    test_results[name] = {\n",
        "        \"acc\": te_acc,\n",
        "        \"top5_acc\": te_top5_acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"train_time\": total_train_time,\n",
        "        \"preds\": te_preds,\n",
        "        \"labels\": te_labels\n",
        "    }\n",
        "    all_history[name] = history\n",
        "\n",
        "    # Per-model error analysis (top-5 confused pairs)\n",
        "    errors = [(true, pred) for true, pred in zip(te_labels, te_preds) if true != pred]\n",
        "    if errors:\n",
        "        most_common_errors = Counter(errors).most_common(5)\n",
        "        print(f\"Top 5 confused pairs for {name} (True → Predicted):\")\n",
        "        for (true_label, pred_label), count in most_common_errors:\n",
        "            true_name = class_names[true_label]\n",
        "            pred_name = class_names[pred_label]\n",
        "            print(f\"  {true_name} → {pred_name} : {count} times\")"
      ],
      "metadata": {
        "id": "a1T3PglGO2Mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee0c78f-fad2-4ae5-c78b-171d30ccf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training resnet18 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/tmp/ipython-input-3482151153.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
            "Training:   0%|          | 0/160 [00:00<?, ?it/s]/tmp/ipython-input-838357888.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n",
            "Training:  16%|█▋        | 26/160 [00:09<00:35,  3.72it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# 7. Compare Models\n",
        "# =============================================================================\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "comparison_data = {\n",
        "    \"Model\": list(test_results.keys()),\n",
        "    \"Test Accuracy\": [f\"{test_results[m]['acc']:.4f}\" for m in test_results],\n",
        "    \"Test Top-5 Accuracy\": [f\"{test_results[m]['top5_acc']:.4f}\" for m in test_results],\n",
        "    \"Macro F1-Score\": [f\"{test_results[m]['macro_f1']:.4f}\" for m in test_results],\n",
        "    \"Total Train Time (s)\": [f\"{test_results[m]['train_time']:.1f}\" for m in test_results]\n",
        "}\n",
        "print(pd.DataFrame(comparison_data).to_markdown(index=False))  # Requires import pandas as pd if needed\n",
        "\n",
        "# Plot combined curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "for name in all_history:\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(all_history[name][\"train_loss\"], label=f\"{name} train_loss\")\n",
        "    plt.plot(all_history[name][\"val_loss\"], label=f\"{name} val_loss\")\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(all_history[name][\"train_acc\"], label=f\"{name} train_acc\")\n",
        "    plt.plot(all_history[name][\"val_acc\"], label=f\"{name} val_acc\")\n",
        "    plt.title(\"Accuracy Curves\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AdZaCpN4Ns5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 8. Plot Confusion Matrices for Both Models\n",
        "# =============================================================================\n",
        "print(\"\\n=== Plotting Confusion Matrices ===\")\n",
        "\n",
        "for name in test_results:\n",
        "    y_true = test_results[name]['labels']   # list or np.array of true labels\n",
        "    y_pred = test_results[name]['preds']    # list or np.array of predicted labels\n",
        "\n",
        "    # Compute Confusion Matrice (200x200)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Normalized (row-normalized to 1, easier to see error distribution)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    cm_normalized = np.nan_to_num(cm_normalized)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 9))\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        cmap='Blues',\n",
        "        annot=False,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        square=False,\n",
        "        cbar_kws={'label': 'Normalized', 'shrink': 0.7}\n",
        "    )\n",
        "    plt.title(f'Normalized Confusion Matrix - {name.upper()}', fontsize=12)\n",
        "    plt.ylabel('True', fontsize=10)\n",
        "    plt.xlabel('Predicted', fontsize=10)\n",
        "    plt.tight_layout(pad=0.5)\n",
        "    plt.savefig(f'cm_{name}_small.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2cuyAi0hgupW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}